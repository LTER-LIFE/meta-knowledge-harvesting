{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd0852d",
   "metadata": {},
   "source": [
    "# LLM-for-Metadata-Harvesting\n",
    "\n",
    "This notebook contains the experimental results from [P6: Groot zeegras (2023)](https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/TF1TbsTxTqykP5rv6MXJEg).  \n",
    "The results can be found under the last code block. Note that not all code is directly relevant to this experiment; some parts are retained for future development and elaboration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d59c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip to main content\n",
      "\n",
      "developers.google.com uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more.\n",
      "\n",
      "OK, got it\n",
      "Earth Engine Data Catalog\n",
      "/\n",
      "Sign in\n",
      "Home\n",
      "Categories\n",
      "All datasets\n",
      "All tags\n",
      "Landsat\n",
      "MODIS\n",
      "Sentinel\n",
      "Publisher\n",
      "Community\n",
      "API Docs\n",
      "HLSS30: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m \n",
      "bookmark_border\n",
      "Dataset Availability\n",
      "2015-11-28T00:00:00Z–2025-05-06T23:38:31Z\n",
      "Dataset Provider\n",
      "NASA LP DAAC\n",
      "Earth Engine Snippet\n",
      "ee.ImageCollection(\"NASA/HLS/HLSS30/v002\") open_in_new\n",
      "Tags\n",
      "landsat nasa satellite-imagery sentinel usgs\n",
      "Description\n",
      "Bands\n",
      "Image Properties\n",
      "Terms of Use\n",
      "Citations\n",
      "DOIs\n",
      "\n",
      "The Harmonized Landsat Sentinel-2 (HLS) project provides consistent surface reflectance data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe's Copernicus Sentinel-2A satellites. The combined measurement enables global observations of the land every 2-3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\n",
      "\n",
      "The HLS project distributes data as two separate products: HLSL30 (Landsat 8/9) and HLSS30 (Sentinel-2 A/B). They both provide 30m Nadir Bidirectional Reflectance Distribution Function (BRDF), Adjusted Reflectance (NBAR).\n",
      "\n",
      "Documentation:\n",
      "\n",
      "User's Guide\n",
      "\n",
      "Algorithm Theoretical Basis Document (ATBD)\n",
      "\n",
      "General Documentation\n",
      "\n",
      "L30 catalog link: NASA/HLS/HLSL30/v002\n",
      "\n",
      "Note: The historical assets are currently being ingested and this process is expected to finish in 2025.\n",
      "Explore with Earth Engine\n",
      "Important: Earth Engine is a platform for petabyte-scale scientific analysis and visualization of geospatial datasets, both for public benefit and for business and government users. Earth Engine is free to use for research, education, and nonprofit use. To get started, please register for Earth Engine access.\n",
      "Code Editor (JavaScript)\n",
      "var collection = ee.ImageCollection(\"NASA/HLS/HLSS30/v002\")\n",
      "                    .filter(ee.Filter.date('2024-04-25', '2024-04-26'))\n",
      "                    .filter(ee.Filter.lt('CLOUD_COVERAGE', 30));\n",
      "var visParams = {\n",
      "  bands: ['B4', 'B3', 'B2'],\n",
      "  min:0.01,\n",
      "  max:0.18,\n",
      "};\n",
      "\n",
      "var visualizeImage = function(image) {\n",
      "  var imageRGB = image.visualize(visParams);\n",
      "  return imageRGB;\n",
      "};\n",
      "\n",
      "var rgbCollection = collection.map(visualizeImage);\n",
      "\n",
      "Map.setCenter(-109.53, 29.19, 12)\n",
      "Map.addLayer(rgbCollection, {}, 'HLS S30 RGB bands');\n",
      "Open in Code Editor\n",
      "GitHub\n",
      "Earth Engine on GitHub\n",
      "Medium\n",
      "Follow our blog on Medium\n",
      "GIS Stack Exchange\n",
      "Ask questions using the google-earth-engine tag\n",
      "Twitter\n",
      "Follow @googleearth on Twitter\n",
      "Videos\n",
      "Earth Engine on YouTube\n",
      "Connect\n",
      "Blog\n",
      "Instagram\n",
      "LinkedIn\n",
      "X (Twitter)\n",
      "YouTube\n",
      "Programs\n",
      "Google Developer Groups\n",
      "Google Developer Experts\n",
      "Accelerators\n",
      "Women Techmakers\n",
      "Google Cloud & NVIDIA\n",
      "Developer consoles\n",
      "Google API Console\n",
      "Google Cloud Platform Console\n",
      "Google Play Console\n",
      "Firebase Console\n",
      "Actions on Google Console\n",
      "Cast SDK Developer Console\n",
      "Chrome Web Store Dashboard\n",
      "Google Home Developer Console\n",
      "Android\n",
      "Chrome\n",
      "Firebase\n",
      "Google Cloud Platform\n",
      "Google AI\n",
      "All products\n",
      "Terms\n",
      "Privacy\n",
      "Sign up for the Google for Developers newsletter\n",
      "Subscribe\n",
      "Info\n",
      "Chat\n",
      "API\n"
     ]
    }
   ],
   "source": [
    "from cheatsheet import CHEATSHEETS\n",
    "from prompt import PROMPTS\n",
    "from grobidmonkey import reader\n",
    "from webutils import readWebContent, downloadAndParseXML\n",
    "\n",
    "monkeyReader = reader.MonkeyReader('monkey') # or 'lxml' or 'x2d'\n",
    "\n",
    "dataPortalURL = [\n",
    "    \"https://developers.google.com/earth-engine/datasets/catalog/NASA_HLS_HLSS30_v002\",\n",
    "    \"https://lpdaac.usgs.gov/products/mod09a1v061/\",\n",
    "    \"https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en\",\n",
    "    \"https://stac.ecodatacube.eu/ndvi_glad.landsat.ard2.seasconv/collection.json?.language=en\",\n",
    "    \"https://zenodo.org/records/8319440\",\n",
    "    \"https://lifesciences.datastations.nl/dataset.xhtml?persistentId=doi:10.17026/dans-2bd-kskz\",\n",
    "    \"https://www.gbif.org/dataset/4fa7b334-ce0d-4e88-aaae-2e0c138d049e\",\n",
    "    \"https://www.gbif.org/dataset/74196cd9-7ebc-4b20-bc27-3c2d22e31ed7\",\n",
    "    \"https://www.gbif.org/dataset/bc0acb9a-131f-4085-93ae-a46e08564ac5\",\n",
    "    \"https://zenodo.org/records/11440456\",\n",
    "    \"https://stac.ecodatacube.eu/blue_glad.landsat.ard2.seasconv.m.yearly/collection.json\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/eng/catalog.search#/metadata/L-mHomzGRuKAHGMkUPjY9g\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/eng/catalog.search#/metadata/0fe7e64b-50b3-4cee-b64a-02659fc2b6c7\",\n",
    "    \"https://stac.ecodatacube.eu/green_glad.landsat.ard2.seasconv.m.yearly/collection.json\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/A0h06_NlSEuNlium5OO3FA\",\n",
    "]\n",
    "\n",
    "# Get the web content\n",
    "url = dataPortalURL[0]\n",
    "\n",
    "\n",
    "# soup = readWebContent(url)\n",
    "# if soup is None:\n",
    "#     raise ValueError(\"Failed to retrieve web content\")\n",
    "\n",
    "# # Extract text from the webpage - adjust the selector based on the webpage structure\n",
    "# # This is a basic example - you might need to modify based on the specific webpage\n",
    "# text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "# text_xml, _ = downloadAndParseXML(\"https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/A0h06_NlSEuNlium5OO3FA/formatters/xml\")\n",
    "# text += \"\\n\" + text_xml\n",
    "# full_text = text\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from webutils import extract_full_page_text\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "full_text = await extract_full_page_text(url)\n",
    "\n",
    "# Optionally display or save it\n",
    "print(full_text)  # Print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91c5532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [19:08<00:00, 76.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from hearvester_operations import extract_entities\n",
    "initial_url_map = {}\n",
    "clean_url_map = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for url in tqdm(dataPortalURL):\n",
    "    index += 1\n",
    "    # Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Run the async function\n",
    "    if url.startswith(\"https://lpdaac.usgs.gov\"):\n",
    "        soup = readWebContent(url)\n",
    "        if soup is None:\n",
    "            raise ValueError(\"Failed to retrieve web content\")\n",
    "\n",
    "        # Extract text from the webpage - adjust the selector based on the webpage structure\n",
    "        # This is a basic example - you might need to modify based on the specific webpage\n",
    "        full_text = soup.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        full_text = await extract_full_page_text(url)\n",
    "\n",
    "    # Optionally display or save it\n",
    "    special_interest = CHEATSHEETS.get(\"special_interests\", \"Focus on metadata fields and their relationships\")\n",
    "    initial_nodes, clean_nodes = extract_entities(\n",
    "        text=full_text, \n",
    "        entity_types=PROMPTS[\"DEFAULT_ENTITY_TYPES\"], \n",
    "        special_interest=special_interest\n",
    "    )\n",
    "\n",
    "    initial_entity_type_map = {}\n",
    "\n",
    "    for entity_group in initial_nodes.values():\n",
    "        for item in entity_group:\n",
    "            entity_name = item.get('entity_name')\n",
    "            entity_type = item.get('entity_type')\n",
    "            entity_description = item.get('description')\n",
    "\n",
    "            # Initialize the list for this entity_type if not already present\n",
    "            if entity_name not in initial_entity_type_map:\n",
    "                initial_entity_type_map[entity_type] = []\n",
    "\n",
    "            # Append the (entity_name, description) pair\n",
    "            initial_entity_type_map[entity_type].append(entity_name + '; ' + entity_description)\n",
    "    initial_url_map[url] = initial_entity_type_map\n",
    "\n",
    "    # Create a dictionary to store entity_type: [(entity_name, description), ...]\n",
    "    clean_entity_type_map = {}\n",
    "\n",
    "    for entity_group in clean_nodes.values():\n",
    "        for item in entity_group:\n",
    "            entity_name = item.get('entity_name')\n",
    "            entity_value = item.get('entity_value')\n",
    "\n",
    "            # Initialize the list for this entity_type if not already present\n",
    "            if entity_name not in clean_entity_type_map:\n",
    "                clean_entity_type_map[entity_name] = []\n",
    "\n",
    "            # Append the (entity_name, description) pair\n",
    "            clean_entity_type_map[entity_name].append(entity_value)\n",
    "    clean_url_map[url] = clean_entity_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5af097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "output_file_path = \"outputs/\" + date_str + \"/\" + \"clean_entity_type_map.yaml\"\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "# Sort the inner dictionary keys for each dataset URL\n",
    "sorted_data = {\n",
    "    url: dict(sorted(fields.items()))\n",
    "    for url, fields in clean_url_map.items()\n",
    "}\n",
    "\n",
    "# Save to YAML\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    yaml.dump(sorted_data, file, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "output_file_path = \"outputs/\" + date_str + \"/\" + \"initial_entity_type_map.yaml\"\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "# Sort the inner dictionary keys for each dataset URL\n",
    "sorted_data = {\n",
    "    url: dict(sorted(fields.items()))\n",
    "    for url, fields in initial_url_map.items()\n",
    "}\n",
    "\n",
    "# Save to YAML\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    yaml.dump(sorted_data, file, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata_block(text: str) -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Parse a metadata block and return the source URL and a dictionary of metadata fields.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text block containing metadata information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (source_url, metadata_dict) where metadata_dict contains field types as keys \n",
    "        and lists of (value, description) tuples as values\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    metadata = {}\n",
    "    source_url = \"\"\n",
    "    current_field = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines and separator lines\n",
    "        if not line or line.startswith('==='):\n",
    "            continue\n",
    "            \n",
    "        # Extract source URL\n",
    "        if line.startswith('Source URL:'):\n",
    "            source_url = line.replace('Source URL:', '').strip()\n",
    "            continue\n",
    "            \n",
    "        # Check for field type\n",
    "        if line and not line.startswith('-'):\n",
    "            if line.endswith(':'):\n",
    "                current_field = line[:-1]  # Remove trailing colon\n",
    "                metadata[current_field] = []\n",
    "            continue\n",
    "            \n",
    "        # Process metadata entries\n",
    "        if line.startswith('- (') and current_field:\n",
    "            # Extract content between parentheses\n",
    "            content = line[5:-1]  # Remove \"  - (\" and \")\"\n",
    "            if ',' in content:\n",
    "                # Split into value and description\n",
    "                value, desc = content.split(',', 1)\n",
    "                value = value.strip()\n",
    "                desc = desc.strip()\n",
    "                metadata[current_field].append((value, desc))\n",
    "    \n",
    "    return source_url, metadata\n",
    "\n",
    "def read_metadata_file(file_path: str) -> list[tuple[str, dict]]:\n",
    "    \"\"\"\n",
    "    Read the metadata file and parse all blocks.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the metadata file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (source_url, metadata_dict) tuples for each block\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split content into blocks using the separator\n",
    "    sep_blocks = content.split('\\n==================================================\\n')\n",
    "    blocks = []\n",
    "\n",
    "    print(len(sep_blocks))\n",
    "\n",
    "    if len(sep_blocks)%2:\n",
    "        for i in range(1, len(sep_blocks)-1, 2):\n",
    "            block = sep_blocks[i] + \"\\n\" + sep_blocks[i+1]\n",
    "            blocks.append(block)\n",
    "    else:\n",
    "        raise ValueError(\"The file content does not have the expected format.\")\n",
    "\n",
    "    \n",
    "    # Parse each non-empty block\n",
    "    results = []\n",
    "    for block in blocks:\n",
    "        if block.strip():\n",
    "            url, metadata = parse_metadata_block(block)\n",
    "            if url:  # Only add blocks with a valid source URL\n",
    "                results.append((url, metadata))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"outputs/2025-05-20entity_type_map.txt\"\n",
    "metadata_blocks = read_metadata_file(file_path)\n",
    "\n",
    "# Print example of first block\n",
    "if metadata_blocks:\n",
    "    url, metadata = metadata_blocks[0]\n",
    "    print(f\"Source URL: {url}\")\n",
    "    print(\"\\nExample fields:\")\n",
    "    for field, values in list(metadata.items())[:3]:  # Show first 3 fields\n",
    "        print(f\"\\n{field}:\")\n",
    "        for value, desc in values:\n",
    "            print(f\"  - {value}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7941bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file created using dill: my_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# create_dill_pickle.py\n",
    "import dill\n",
    "\n",
    "class MyModel:\n",
    "    def get_team(self):\n",
    "        return \"Awesome Hackers\"\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Save with dill\n",
    "with open(\"my_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f, protocol=4)\n",
    "\n",
    "print(\"Pickle file created using dill: my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e05890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in /home/com3dian/miniconda3/envs/sam2/lib/python3.12/site-packages (0.3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11beba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sma2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
