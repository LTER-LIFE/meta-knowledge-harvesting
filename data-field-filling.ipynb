{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd0852d",
   "metadata": {},
   "source": [
    "# LLM-for-Metadata-Harvesting\n",
    "\n",
    "This notebook contains the experimental results from [P6: Groot zeegras (2023)](https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/TF1TbsTxTqykP5rv6MXJEg).  \n",
    "The results can be found under the last code block. Note that not all code is directly relevant to this experiment; some parts are retained for future development and elaboration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d59c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip to main content\n",
      "\n",
      "developers.google.com uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more.\n",
      "\n",
      "OK, got it\n",
      "Earth Engine Data Catalog\n",
      "/\n",
      "Sign in\n",
      "Home\n",
      "Categories\n",
      "All datasets\n",
      "All tags\n",
      "Landsat\n",
      "MODIS\n",
      "Sentinel\n",
      "Publisher\n",
      "Community\n",
      "API Docs\n",
      "HLSS30: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m \n",
      "bookmark_border\n",
      "Dataset Availability\n",
      "2015-11-28T00:00:00Zâ€“2025-05-06T23:38:31Z\n",
      "Dataset Provider\n",
      "NASA LP DAAC\n",
      "Earth Engine Snippet\n",
      "ee.ImageCollection(\"NASA/HLS/HLSS30/v002\") open_in_new\n",
      "Tags\n",
      "landsat nasa satellite-imagery sentinel usgs\n",
      "Description\n",
      "Bands\n",
      "Image Properties\n",
      "Terms of Use\n",
      "Citations\n",
      "DOIs\n",
      "\n",
      "The Harmonized Landsat Sentinel-2 (HLS) project provides consistent surface reflectance data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe's Copernicus Sentinel-2A satellites. The combined measurement enables global observations of the land every 2-3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\n",
      "\n",
      "The HLS project distributes data as two separate products: HLSL30 (Landsat 8/9) and HLSS30 (Sentinel-2 A/B). They both provide 30m Nadir Bidirectional Reflectance Distribution Function (BRDF), Adjusted Reflectance (NBAR).\n",
      "\n",
      "Documentation:\n",
      "\n",
      "User's Guide\n",
      "\n",
      "Algorithm Theoretical Basis Document (ATBD)\n",
      "\n",
      "General Documentation\n",
      "\n",
      "L30 catalog link: NASA/HLS/HLSL30/v002\n",
      "\n",
      "Note: The historical assets are currently being ingested and this process is expected to finish in 2025.\n",
      "Explore with Earth Engine\n",
      "Important: Earth Engine is a platform for petabyte-scale scientific analysis and visualization of geospatial datasets, both for public benefit and for business and government users. Earth Engine is free to use for research, education, and nonprofit use. To get started, please register for Earth Engine access.\n",
      "Code Editor (JavaScript)\n",
      "var collection = ee.ImageCollection(\"NASA/HLS/HLSS30/v002\")\n",
      "                    .filter(ee.Filter.date('2024-04-25', '2024-04-26'))\n",
      "                    .filter(ee.Filter.lt('CLOUD_COVERAGE', 30));\n",
      "var visParams = {\n",
      "  bands: ['B4', 'B3', 'B2'],\n",
      "  min:0.01,\n",
      "  max:0.18,\n",
      "};\n",
      "\n",
      "var visualizeImage = function(image) {\n",
      "  var imageRGB = image.visualize(visParams);\n",
      "  return imageRGB;\n",
      "};\n",
      "\n",
      "var rgbCollection = collection.map(visualizeImage);\n",
      "\n",
      "Map.setCenter(-109.53, 29.19, 12)\n",
      "Map.addLayer(rgbCollection, {}, 'HLS S30 RGB bands');\n",
      "Open in Code Editor\n",
      "GitHub\n",
      "Earth Engine on GitHub\n",
      "Medium\n",
      "Follow our blog on Medium\n",
      "GIS Stack Exchange\n",
      "Ask questions using the google-earth-engine tag\n",
      "Twitter\n",
      "Follow @googleearth on Twitter\n",
      "Videos\n",
      "Earth Engine on YouTube\n",
      "Connect\n",
      "Blog\n",
      "Instagram\n",
      "LinkedIn\n",
      "X (Twitter)\n",
      "YouTube\n",
      "Programs\n",
      "Google Developer Groups\n",
      "Google Developer Experts\n",
      "Accelerators\n",
      "Women Techmakers\n",
      "Google Cloud & NVIDIA\n",
      "Developer consoles\n",
      "Google API Console\n",
      "Google Cloud Platform Console\n",
      "Google Play Console\n",
      "Firebase Console\n",
      "Actions on Google Console\n",
      "Cast SDK Developer Console\n",
      "Chrome Web Store Dashboard\n",
      "Google Home Developer Console\n",
      "Android\n",
      "Chrome\n",
      "Firebase\n",
      "Google Cloud Platform\n",
      "Google AI\n",
      "All products\n",
      "Terms\n",
      "Privacy\n",
      "Sign up for the Google for Developers newsletter\n",
      "Subscribe\n",
      "Info\n",
      "Chat\n",
      "API\n"
     ]
    }
   ],
   "source": [
    "from cheatsheet import CHEATSHEETS\n",
    "from prompt import PROMPTS\n",
    "from grobidmonkey import reader\n",
    "from webutils import readWebContent, downloadAndParseXML\n",
    "\n",
    "monkeyReader = reader.MonkeyReader('monkey') # or 'lxml' or 'x2d'\n",
    "\n",
    "dataPortalURL = [\n",
    "    \"https://developers.google.com/earth-engine/datasets/catalog/NASA_HLS_HLSS30_v002\",\n",
    "    \"https://lpdaac.usgs.gov/products/mod09a1v061/\",\n",
    "    \"https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en\",\n",
    "    \"https://stac.ecodatacube.eu/ndvi_glad.landsat.ard2.seasconv/collection.json?.language=en\",\n",
    "    \"https://zenodo.org/records/8319440\",\n",
    "    \"https://lifesciences.datastations.nl/dataset.xhtml?persistentId=doi:10.17026/dans-2bd-kskz\",\n",
    "    \"https://www.gbif.org/dataset/4fa7b334-ce0d-4e88-aaae-2e0c138d049e\",\n",
    "    \"https://www.gbif.org/dataset/74196cd9-7ebc-4b20-bc27-3c2d22e31ed7\",\n",
    "    \"https://www.gbif.org/dataset/bc0acb9a-131f-4085-93ae-a46e08564ac5\",\n",
    "    \"https://zenodo.org/records/11440456\",\n",
    "    \"https://stac.ecodatacube.eu/blue_glad.landsat.ard2.seasconv.m.yearly/collection.json\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/eng/catalog.search#/metadata/L-mHomzGRuKAHGMkUPjY9g\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/eng/catalog.search#/metadata/0fe7e64b-50b3-4cee-b64a-02659fc2b6c7\",\n",
    "    \"https://stac.ecodatacube.eu/green_glad.landsat.ard2.seasconv.m.yearly/collection.json\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/A0h06_NlSEuNlium5OO3FA\",\n",
    "]\n",
    "\n",
    "# Get the web content\n",
    "url = dataPortalURL[0]\n",
    "\n",
    "\n",
    "# soup = readWebContent(url)\n",
    "# if soup is None:\n",
    "#     raise ValueError(\"Failed to retrieve web content\")\n",
    "\n",
    "# # Extract text from the webpage - adjust the selector based on the webpage structure\n",
    "# # This is a basic example - you might need to modify based on the specific webpage\n",
    "# text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "# text_xml, _ = downloadAndParseXML(\"https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/A0h06_NlSEuNlium5OO3FA/formatters/xml\")\n",
    "# text += \"\\n\" + text_xml\n",
    "# full_text = text\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from webutils import extract_full_page_text\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "full_text = await extract_full_page_text(url)\n",
    "\n",
    "# Optionally display or save it\n",
    "print(full_text)  # Print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a68c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from utils import (\n",
    "    logger,\n",
    "    clean_str,\n",
    "    compute_mdhash_id,\n",
    "    decode_tokens_by_tiktoken,\n",
    "    encode_string_by_tiktoken,\n",
    "    is_float_regex,\n",
    "    normalize_extracted_info,\n",
    "    pack_user_ass_to_openai_messages,\n",
    "    split_string_by_multi_markers,\n",
    "    use_llm_func_with_cache,\n",
    ")\n",
    "from collections import defaultdict\n",
    "\n",
    "import tiktoken\n",
    "import re\n",
    "import os\n",
    "\n",
    "llm_model = \"gpt-4\"\n",
    "load_dotenv()\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 6000) -> list[str]:\n",
    "    \"\"\"Split text into chunks that fit within token limit\"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(llm_model)\n",
    "    tokens = encoder.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if current_length + 1 > max_tokens:\n",
    "            # Convert chunk back to text\n",
    "            chunk_text = encoder.decode(current_chunk)\n",
    "            chunks.append(chunk_text)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_chunk.append(token)\n",
    "        current_length += 1\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(encoder.decode(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_entities(text: str, entity_types: list[str], special_interest: str = \"\") -> dict:\n",
    "    # Split text into chunks\n",
    "    chunks = chunk_text(text, max_tokens=4000)  # Leave room for completion\n",
    "    \n",
    "    all_nodes = defaultdict(list)\n",
    "    all_edges = defaultdict(list)\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    nightly_entities_prompt = CHEATSHEETS[\"nightly_entity_template\"].format(\n",
    "        tuple_delimiter=PROMPTS[\"DEFAULT_TUPLE_DELIMITER\"],\n",
    "        record_delimiter=PROMPTS[\"DEFAULT_RECORD_DELIMITER\"],\n",
    "    )\n",
    "\n",
    "    all_records = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in chunks:\n",
    "        formatted_prompt = {\n",
    "            \"language\": \"English\",\n",
    "            \"tuple_delimiter\": PROMPTS[\"DEFAULT_TUPLE_DELIMITER\"],\n",
    "            \"record_delimiter\": PROMPTS[\"DEFAULT_RECORD_DELIMITER\"],\n",
    "            \"completion_delimiter\": PROMPTS[\"DEFAULT_COMPLETION_DELIMITER\"],\n",
    "            \"entity_types\": entity_types,\n",
    "            \"special_interest\": special_interest,\n",
    "            \"nightly_entities\": nightly_entities_prompt,\n",
    "            \"input_text\": chunk\n",
    "        }\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI trained to extract entities (meta data fields) and relationships from text.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": _format_prompt(formatted_prompt)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "\n",
    "        print(\"----------------\\nresponse.choices[0].message.content:\\n\", response.choices[0].message.content)\n",
    "        \n",
    "        # Process the chunk results\n",
    "        records = _process_extraction_result(\n",
    "            response.choices[0].message.content,\n",
    "            chunk_key=compute_mdhash_id(chunk),\n",
    "            file_path=\"unknown_source\"\n",
    "        )\n",
    "        all_records += records\n",
    "    \n",
    "    return _post_processing_records(\n",
    "        all_records, \n",
    "        chunk_key=\"unknown_chunk\", \n",
    "        file_path=\"unknown_source\"\n",
    "    )\n",
    "    \n",
    "\n",
    "def _format_prompt(params: dict) -> str:\n",
    "    # Format the prompt template with the provided parameters\n",
    "    prompt_template = CHEATSHEETS[\"fill_nightly\"]\n",
    "    return prompt_template.format(**params)\n",
    "\n",
    "def _handle_post_processed_entity_extraction(\n",
    "    record_attributes: list[str],\n",
    "    chunk_key: str,\n",
    "    file_path: str = \"unknown_source\",\n",
    "): \n",
    "    \"\"\"Handle the extraction of a single entity from the record attributes.\n",
    "    \n",
    "    Args:\n",
    "        record_attributes (list[str]): The attributes of the record to process\n",
    "        chunk_key (str): The key for the chunk being processed\n",
    "        file_path (str): The file path for citation\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted entity information, or None if extraction fails\n",
    "    \"\"\"\n",
    "    if len(record_attributes) < 3 or record_attributes[0] != '\"entity\"':\n",
    "        return None\n",
    "\n",
    "    # Clean and validate entity name\n",
    "    entity_name = clean_str(record_attributes[1]).strip('\"')\n",
    "    if not entity_name.strip():\n",
    "        logger.warning(\n",
    "            f\"Entity extraction error: empty entity name in: {record_attributes}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Normalize entity name\n",
    "    entity_name = normalize_extracted_info(entity_name, is_entity=True)\n",
    "\n",
    "    # Clean and validate entity type\n",
    "    entity_value = clean_str(record_attributes[2]).strip('\"')\n",
    "    if not entity_value.strip() or entity_value.startswith('(\"'):\n",
    "        logger.warning(\n",
    "            f\"Entity extraction error: invalid entity type in: {record_attributes}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    return dict(\n",
    "        entity_name=entity_name,\n",
    "        entity_value=entity_value,\n",
    "        source_id=chunk_key,\n",
    "        file_path=file_path,\n",
    "    )\n",
    "\n",
    "def _handle_single_entity_extraction(\n",
    "    record_attributes: list[str],\n",
    "    chunk_key: str,\n",
    "    file_path: str = \"unknown_source\",\n",
    "):\n",
    "    if len(record_attributes) < 4 or record_attributes[0] != '\"entity\"':\n",
    "        return None\n",
    "\n",
    "    # Clean and validate entity name\n",
    "    entity_name = clean_str(record_attributes[1]).strip('\"')\n",
    "    if not entity_name.strip():\n",
    "        logger.warning(\n",
    "            f\"Entity extraction error: empty entity name in: {record_attributes}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Normalize entity name\n",
    "    entity_name = normalize_extracted_info(entity_name, is_entity=True)\n",
    "\n",
    "    # Clean and validate entity type\n",
    "    entity_type = clean_str(record_attributes[2]).strip('\"')\n",
    "    if not entity_type.strip() or entity_type.startswith('(\"'):\n",
    "        logger.warning(\n",
    "            f\"Entity extraction error: invalid entity type in: {record_attributes}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Clean and validate description\n",
    "    entity_description = clean_str(record_attributes[3])\n",
    "    entity_description = normalize_extracted_info(entity_description)\n",
    "\n",
    "    if not entity_description.strip():\n",
    "        logger.warning(\n",
    "            f\"Entity extraction error: empty description for entity '{entity_name}' of type '{entity_type}'\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    return dict(\n",
    "        entity_name=entity_name,\n",
    "        entity_type=entity_type,\n",
    "        description=entity_description,\n",
    "        source_id=chunk_key,\n",
    "        file_path=file_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def _handle_single_relationship_extraction(\n",
    "    record_attributes: list[str],\n",
    "    chunk_key: str,\n",
    "    file_path: str = \"unknown_source\",\n",
    "):\n",
    "    if len(record_attributes) < 5 or record_attributes[0] != '\"relationship\"':\n",
    "        return None\n",
    "    # add this record as edge\n",
    "    source = clean_str(record_attributes[1])\n",
    "    target = clean_str(record_attributes[2])\n",
    "\n",
    "    # Normalize source and target entity names\n",
    "    source = normalize_extracted_info(source, is_entity=True)\n",
    "    target = normalize_extracted_info(target, is_entity=True)\n",
    "\n",
    "    edge_description = clean_str(record_attributes[3])\n",
    "    edge_description = normalize_extracted_info(edge_description)\n",
    "\n",
    "    edge_keywords = clean_str(record_attributes[4]).strip('\"').strip(\"'\")\n",
    "    edge_source_id = chunk_key\n",
    "    weight = (\n",
    "        float(record_attributes[-1].strip('\"').strip(\"'\"))\n",
    "        if is_float_regex(record_attributes[-1])\n",
    "        else 1.0\n",
    "    )\n",
    "    return dict(\n",
    "        src_id=source,\n",
    "        tgt_id=target,\n",
    "        weight=weight,\n",
    "        description=edge_description,\n",
    "        keywords=edge_keywords,\n",
    "        source_id=edge_source_id,\n",
    "        file_path=file_path,\n",
    "    )\n",
    "\n",
    "def _post_process_single_record(record: str, context_base: dict) -> tuple[str, list[str]]:\n",
    "    \"\"\"Process a single record by cleaning and extracting its contents.\n",
    "    \n",
    "    Args:\n",
    "        record (str): The record string to process\n",
    "        context_base (dict): Dictionary containing delimiter configuration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (processed_record, record_attributes) where:\n",
    "            - processed_record is the cleaned record string\n",
    "            - record_attributes is a list of attributes split by delimiter\n",
    "    \"\"\"\n",
    "    # Add parentheses if they don't exist\n",
    "    if not record.startswith('('):\n",
    "        record = f'({record})'\n",
    "    if not record.endswith(')'):\n",
    "        record = f'{record})'\n",
    "        \n",
    "    # Extract content between parentheses\n",
    "    match = re.search(r\"\\((.*)\\)\", record)\n",
    "    if match is None:\n",
    "        print(f\"Record extraction error: invalid record format in: {record}\")\n",
    "        return None, []\n",
    "        \n",
    "    processed_record = match.group(1)\n",
    "    record_attributes = split_string_by_multi_markers(\n",
    "        processed_record, \n",
    "        [context_base[\"tuple_delimiter\"]]\n",
    "    )\n",
    "    \n",
    "    return processed_record, record_attributes\n",
    "\n",
    "def _process_extraction_result(\n",
    "        result: str, chunk_key: str, file_path: str = \"unknown_source\"\n",
    "    ):\n",
    "        \"\"\"Process a single extraction result (either initial or gleaning)\n",
    "        Args:\n",
    "            result (str): The extraction result to process\n",
    "            chunk_key (str): The chunk key for source tracking\n",
    "            file_path (str): The file path for citation\n",
    "        Returns:\n",
    "            tuple: (nodes_dict, edges_dict) containing the extracted entities and relationships\n",
    "        \"\"\"\n",
    "        context_base = dict(\n",
    "            tuple_delimiter=PROMPTS[\"DEFAULT_TUPLE_DELIMITER\"],\n",
    "            record_delimiter=PROMPTS[\"DEFAULT_RECORD_DELIMITER\"],\n",
    "            completion_delimiter=PROMPTS[\"DEFAULT_COMPLETION_DELIMITER\"],\n",
    "        )\n",
    "        maybe_nodes = defaultdict(list)\n",
    "        maybe_edges = defaultdict(list)\n",
    "\n",
    "        records = split_string_by_multi_markers(\n",
    "            result,\n",
    "            [context_base[\"record_delimiter\"], context_base[\"completion_delimiter\"], \"\\n\"],\n",
    "        )\n",
    "        return records\n",
    "\n",
    "def _post_processing_records(all_records: list[str], chunk_key: str, file_path: str = \"unknown_source\"):\n",
    "    \"\"\"Post-process records to extract entities and relationships.\n",
    "    \n",
    "    This function processes the extracted records, cleaning them and extracting\n",
    "    entities and relationships based on predefined rules.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (maybe_nodes, maybe_edges) where:\n",
    "            - maybe_nodes is a dictionary of extracted entities\n",
    "            - maybe_edges is a dictionary of extracted relationships\n",
    "    \"\"\"\n",
    "    context_base = dict(\n",
    "        tuple_delimiter=PROMPTS[\"DEFAULT_TUPLE_DELIMITER\"],\n",
    "        record_delimiter=PROMPTS[\"DEFAULT_RECORD_DELIMITER\"],\n",
    "        completion_delimiter=PROMPTS[\"DEFAULT_COMPLETION_DELIMITER\"],\n",
    "    )\n",
    "    \n",
    "    maybe_nodes = defaultdict(list)\n",
    "\n",
    "    merged_records = \"\"\n",
    "    for record in all_records:\n",
    "        processed_record, record_attributes = _post_process_single_record(record, context_base)\n",
    "        if processed_record is None:\n",
    "            continue\n",
    "        \n",
    "        if_entities = _handle_single_entity_extraction(\n",
    "            record_attributes, chunk_key=\"unknown_chunk\", file_path=\"unknown_source\"\n",
    "        )\n",
    "        if if_entities is not None:\n",
    "            entity_type = if_entities[\"entity_type\"]\n",
    "            entity_description = if_entities[\"description\"]\n",
    "            entity_name = if_entities[\"entity_name\"]\n",
    "            maybe_nodes[entity_type].append(if_entities)\n",
    "            continue\n",
    "    \n",
    "    for entity_type, entities in maybe_nodes.items():\n",
    "        entity_type = entity_type.strip('\"')\n",
    "        entity_description = \"\"\n",
    "        for entity in entities:\n",
    "            entity_description += entity[\"entity_name\"] + \". \" + entity[\"description\"]\n",
    "        \n",
    "        merged_records += f\"(\\\"entity\\\"{context_base['tuple_delimiter']}{entity_type}{context_base['tuple_delimiter']}{entity_description}){context_base['record_delimiter']}\\n\"\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI trained to extract entities (meta data fields) and relationships from text.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": CHEATSHEETS[\"post_processing\"].format(\n",
    "                    language=\"English\",\n",
    "                    tuple_delimiter=context_base[\"tuple_delimiter\"],\n",
    "                    record_delimiter=context_base[\"record_delimiter\"],\n",
    "                    completion_delimiter=context_base[\"record_delimiter\"],\n",
    "                    input_entities=merged_records,\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "\n",
    "    records = split_string_by_multi_markers(\n",
    "        result,\n",
    "        [context_base[\"record_delimiter\"], context_base[\"completion_delimiter\"], \"\\n\"],\n",
    "    )\n",
    "\n",
    "    print(\"----------------\\nresponse.choices[0].message.content:\\n\", result)\n",
    "    final_nodes = defaultdict(list)\n",
    "\n",
    "    for record in records:\n",
    "        print(f\"Processing record: {record}\")\n",
    "        # Add parentheses if they don't exist\n",
    "        if not record.startswith('('):\n",
    "            record = f'({record})'\n",
    "        if not record.endswith(')'):\n",
    "            record = f'{record})'\n",
    "        record = re.search(r\"\\((.*)\\)\", record)\n",
    "        if record is None:\n",
    "            print(\n",
    "                f\"Record extraction error: invalid record format in: {record}\"\n",
    "            )\n",
    "            continue\n",
    "        record = record.group(1)\n",
    "        record_attributes = split_string_by_multi_markers(\n",
    "            record, [context_base[\"tuple_delimiter\"]]\n",
    "        )\n",
    "\n",
    "        if_entities = _handle_post_processed_entity_extraction(\n",
    "            record_attributes, chunk_key, file_path\n",
    "        )\n",
    "        if if_entities is not None:\n",
    "            final_nodes[if_entities[\"entity_name\"]].append(if_entities)\n",
    "            continue\n",
    "\n",
    "    return final_nodes\n",
    "\n",
    "\n",
    "# special_interest = CHEATSHEETS.get(\"special_interests\", \"Focus on metadata fields and their relationships\")\n",
    "# output_nodes = extract_entities(\n",
    "#     text=full_text, \n",
    "#     entity_types=PROMPTS[\"DEFAULT_ENTITY_TYPES\"], \n",
    "#     special_interest=special_interest\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c5532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      "  - EOD â€“ eBird Observation Dataset, WMR - TMAP: Ecotopenkaart, Wadden viewer, Commission Regulation (EU) No 1089/2010\n",
      "Description:\n",
      "  - eBird is a collective enterprise that develops cooperative partnerships among experts in various fields, aiming to increase data quantity and control for data quality issues. It is a major source of biodiversity data. Voor een goed beheer van gebieden zoals de Waddenzee is een consistent en eenduidig inzicht in de veranderingen van de diverse landschappelijke eenheden van groot belang. This data set is conformant with the INSPIRE Implementing Rules for the interoperability of spatial data sets and services.\n",
      "Unique Identifier:\n",
      "  - A0h06_NlSEuNlium5OO3FA, Data afkomstig van WMR tbv TMAP\n",
      "Metadata language:\n",
      "  - Dutch\n",
      "Responsible organization metadata:\n",
      "  - Wageningen Marine Research\n",
      "Data contact point:\n",
      "  - info@wur.nl\n",
      "Resource type:\n",
      "  - Dataset\n",
      "Spatial coverage:\n",
      "  - Worldwide, Waddenzee\n",
      "Metadata date:\n",
      "  - 2016-11-01, 2010-12-08\n",
      "Landing page:\n",
      "  - https://viewer.openearth.nl/wadden-viewer/?layers=93966870&layerNames=WMR+-+TMAP%3A+Ecotopenkaart&folders=143810679%2C93790346, http://data.europa.eu/eli/reg/2010/1089\n",
      "Keywords:\n",
      "  - TMAP, Trilaterale Waddenzee, UNESCO Werelderfgoed, Natuurwaarde, Ecotopen\n",
      "Data creator:\n",
      "  - Wageningen Marine Research\n",
      "Data publisher:\n",
      "  - Wageningen Marine Research\n",
      "Spatial resolution:\n",
      "  - None\n",
      "Spatial reference system:\n",
      "  - https://standards.iso.org/iso/19139/resources/gmxCodelists.xml#MD_ScopeCode\n",
      "Temporal coverage:\n",
      "  - 2008-06-01\n",
      "Temporal resolution:\n",
      "  - Not specified\n",
      "License:\n",
      "  - Copyright\n",
      "Access rights:\n",
      "  - Geen gebruiks limitaties\n",
      "Distribution access URL:\n",
      "  - https://viewer.openearth.nl/wadden-viewer/download/geoserver?layers=93966870\n",
      "Distribution format:\n",
      "  - gml+xml\n",
      "Distribution byte size:\n",
      "  - Not specified\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for url in tqdm(dataPortalURL):\n",
    "    # Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Run the async function\n",
    "    full_text = await extract_full_page_text(url)\n",
    "\n",
    "    # Optionally display or save it\n",
    "    special_interest = CHEATSHEETS.get(\"special_interests\", \"Focus on metadata fields and their relationships\")\n",
    "    output_nodes = extract_entities(\n",
    "        text=full_text, \n",
    "        entity_types=PROMPTS[\"DEFAULT_ENTITY_TYPES\"], \n",
    "        special_interest=special_interest\n",
    "    )\n",
    "    # Create a dictionary to store entity_type: [(entity_name, description), ...]\n",
    "    entity_type_map = {}\n",
    "\n",
    "    for entity_group in output_nodes.values():\n",
    "        for item in entity_group:\n",
    "            entity_name = item.get('entity_name')\n",
    "            entity_value = item.get('entity_value')\n",
    "\n",
    "            # Initialize the list for this entity_type if not already present\n",
    "            if entity_name not in entity_type_map:\n",
    "                entity_type_map[entity_name] = []\n",
    "\n",
    "            # Append the (entity_name, description) pair\n",
    "            entity_type_map[entity_name].append(entity_value)\n",
    "\n",
    "    # Example: print results and save to a file with separator and URL\n",
    "    # get current date\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    output_file_path = \"outputs/\" + date_str + \"entity_type_map.txt\"\n",
    "\n",
    "    with open(output_file_path, \"a\") as file:  # Use \"a\" to append to the file\n",
    "        separator = \"\\n\" + \"=\" * 50 + \"\\n\"  # Separator for each run\n",
    "        file.write(separator)\n",
    "        file.write(f\"Source URL: {url}\\n\")\n",
    "        file.write(separator)\n",
    "        \n",
    "        for entity_type, values in entity_type_map.items():\n",
    "            print(f\"{entity_type}:\")\n",
    "            file.write(f\"{entity_type}:\\n\")\n",
    "            for value in values:\n",
    "                print(f\"  - {value}\")\n",
    "                file.write(f\"  - {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata_block(text: str) -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Parse a metadata block and return the source URL and a dictionary of metadata fields.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text block containing metadata information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (source_url, metadata_dict) where metadata_dict contains field types as keys \n",
    "        and lists of (value, description) tuples as values\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    metadata = {}\n",
    "    source_url = \"\"\n",
    "    current_field = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines and separator lines\n",
    "        if not line or line.startswith('==='):\n",
    "            continue\n",
    "            \n",
    "        # Extract source URL\n",
    "        if line.startswith('Source URL:'):\n",
    "            source_url = line.replace('Source URL:', '').strip()\n",
    "            continue\n",
    "            \n",
    "        # Check for field type\n",
    "        if line and not line.startswith('-'):\n",
    "            if line.endswith(':'):\n",
    "                current_field = line[:-1]  # Remove trailing colon\n",
    "                metadata[current_field] = []\n",
    "            continue\n",
    "            \n",
    "        # Process metadata entries\n",
    "        if line.startswith('- (') and current_field:\n",
    "            # Extract content between parentheses\n",
    "            content = line[5:-1]  # Remove \"  - (\" and \")\"\n",
    "            if ',' in content:\n",
    "                # Split into value and description\n",
    "                value, desc = content.split(',', 1)\n",
    "                value = value.strip()\n",
    "                desc = desc.strip()\n",
    "                metadata[current_field].append((value, desc))\n",
    "    \n",
    "    return source_url, metadata\n",
    "\n",
    "def read_metadata_file(file_path: str) -> list[tuple[str, dict]]:\n",
    "    \"\"\"\n",
    "    Read the metadata file and parse all blocks.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the metadata file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (source_url, metadata_dict) tuples for each block\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split content into blocks using the separator\n",
    "    sep_blocks = content.split('\\n==================================================\\n')\n",
    "    blocks = []\n",
    "\n",
    "    print(len(sep_blocks))\n",
    "\n",
    "    if len(sep_blocks)%2:\n",
    "        for i in range(1, len(sep_blocks)-1, 2):\n",
    "            block = sep_blocks[i] + \"\\n\" + sep_blocks[i+1]\n",
    "            blocks.append(block)\n",
    "    else:\n",
    "        raise ValueError(\"The file content does not have the expected format.\")\n",
    "\n",
    "    \n",
    "    # Parse each non-empty block\n",
    "    results = []\n",
    "    for block in blocks:\n",
    "        if block.strip():\n",
    "            url, metadata = parse_metadata_block(block)\n",
    "            if url:  # Only add blocks with a valid source URL\n",
    "                results.append((url, metadata))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"outputs/2025-05-20entity_type_map.txt\"\n",
    "metadata_blocks = read_metadata_file(file_path)\n",
    "\n",
    "# Print example of first block\n",
    "if metadata_blocks:\n",
    "    url, metadata = metadata_blocks[0]\n",
    "    print(f\"Source URL: {url}\")\n",
    "    print(\"\\nExample fields:\")\n",
    "    for field, values in list(metadata.items())[:3]:  # Show first 3 fields\n",
    "        print(f\"\\n{field}:\")\n",
    "        for value, desc in values:\n",
    "            print(f\"  - {value}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7941bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file created using dill: my_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# create_dill_pickle.py\n",
    "import dill\n",
    "\n",
    "class MyModel:\n",
    "    def get_team(self):\n",
    "        return \"Awesome Hackers\"\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Save with dill\n",
    "with open(\"my_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f, protocol=4)\n",
    "\n",
    "print(\"Pickle file created using dill: my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e05890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in /home/com3dian/miniconda3/envs/sam2/lib/python3.12/site-packages (0.3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11beba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sma2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
